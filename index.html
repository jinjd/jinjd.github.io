<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Jinjd.GitHub.io : MyCould-cloreda">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Jinjd.GitHub.io</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/jinjd">View on GitHub</a>

          <h1 id="project_title">Jinjd.GitHub.io</h1>
          <h2 id="project_tagline">MyCould-cloreda</h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>cloreda笔记
初步感觉，安装很恶心，主要是自动安装不能用，测试后发现是网速问题，所以采用本地源</p>

<p>搭建本地源
wget <a href="http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo">http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo</a>
mv cloudera-cdh5.repo /ect/yum.repo.d/
wget <a href="http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/cloudera-manager.repo">http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/cloudera-manager.repo</a>
mv cloudera-manager.repo /etc/yum.repos.d/</p>

<p>考虑服务器下载各种不给力，vpn不稳定等问题，采用本地虚拟机下载</p>

<p>同步包(与镜像名相同)
reposync -r cloudera-cdh5
reposync -r cloudera-manager
reposync -r cloudera-impala</p>

<p>将目录拷贝到70的wwwroot/cdh下
进入目录，创建源
createrepo .</p>

<p>客户端更新源
yum update</p>

<p>安装cloudera-manager
执行 ./cloudera-manager-installer.bin，本地源就是爽很快安装完成。
使用了默认数据库，值得商榷。</p>

<p>卸载
执行/usr/share/cmf/uninstall-cloudera-manager.sh
rm -rf /var/lib/cloudera-scm-server-db/data</p>

<p>测试默认安装后，以后的安装过程会从网络下载包，为使用本地源使用如下命令
./cloudera-manager-installer.bin --skip_repo_package=1
不太清楚是否管用，更改*.repo文件命名，测试管用</p>

<p>cloudera管理功能非常强大，开始尝试组件集群
规划
70：管理机，软件源
备选机器
71,72,73,74,75,76,87,88,</p>

<p>88,87 namenode 4g 88 mysql
88,87,75,76,74 datanode 2g
88作为模板机</p>

<p>所有集群重新安装系统，开始新的旅程</p>

<p>-------------------------------------------------------2014 2 26-------------------------------------------------------------
安装完成后进行检查，发现两个健康问题
1. Cloudera recommends setting /proc/sys/vm/swappiness to 0. Current setting is 60. Use the sysctl command to change this setting at runtime and edit /etc/sysctl.conf for this setting to be saved after a reboot. You may continue with installation, but you may run into issues with Cloudera Manager reporting that your hosts are unhealthy because they are swapping. The following hosts are affected: </p>

<p>/proc/sys/vm/swappiness 
该文件表示系统进行交换行为的程度，数值（0-100）越高，越可能发生磁盘交换，默认为60
当该参数=0，表示只要有可能就尽力避免交换进程移出物理内存；
当该参数=100，这告诉内核疯狂的将数据移出物理内存移到swap缓存中。
配置/etc/sysctl.conf
    vm.swappiness=0
立即生效
    sysctl –p</p>

<p>已启用“透明大页面”，它可能会导致重大的性能问题。版本为“CentOS release 6.3 (Final)”且版本为“2.6.32-279.el6.x86_64”的 Kernel 已将 enabled 设置为“[always] never”，并将 defrag 设置为“[always] never”。请运行“echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag”以禁用此设置，然后将同一命令添加到一个 init 脚本中，如 /etc/rc.local，这样当系统重启时就会设置它。或者，升级到 RHEL 6.4 或更新版本，它们不存在此错误。将会影响到以下主机
透明超大页面 (THP)。THP 是一个提取层，可自动创建、管理和使用超大页面的大多数方面。</p>

<p>修改/etc/rc.local
    echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/defrag</p>

<p>设置同步时间
    cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
    yum install -y ntpdate
    ntpdate us.pool.ntp.org
    hwclock -w //写入bios，防止重启后时间重置。</p>

<p>周期同步时间，每天同步时间，每周写入bios
crontab -e (分钟，小时，天，月，周)
    0       0       *       *       *       ntpdate us.pool.ntp.org
    0       0       *       *       0       hwclock -w</p>

<p>-------------------------------------------------------2014 2 27-------------------------------------------------------------
启动发现74内存占用比其他机器多，故检查74状态
    top + M 按内存使用情况排序
发现zookeeper占用内存最多，猜测zookeeper可能为管理节点，zookeeper默认选举最后一台机器为管理节点
    echo stat |nc 10.106.1.74 2181 （向10.106.1.74 2181 发送stat命令）</p>

<p>修改内存配置，namenode加大内存
-HADOOP_NAMENODE_OPTS=-Xms699400192 -Xmx699400192 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:OnOutOfMemoryError={{AGENT_COMMON_DIR}}/killparent.sh
5<br>
+HADOOP_NAMENODE_OPTS=-Xms1073741824 -Xmx1073741824 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:OnOutOfMemoryError={{AGENT_COMMON_DIR}}/killparent.sh
6   6<br>
 HADOOP_ROOT_LOGGER=INFO,RFA</p>

<p>提示新的问题，扩大second namenode设置</p>

<p>安装了mapreduce及hbase服务
重启zookeeper，发现所有依赖于zookeeper的服务都自动重启了</p>

<p>关闭agent命令
/etc/init.d/cloudera-scm-agent hard_restart</p>

<p>service cloudera-scm-server restart</p>

<p>添加mysql-java包
将包拷贝到/usr/share/cmf/lib</p>

<p>一直无法使用监控，原来是没添加服务，添加服务管理后就可以进行监控了</p>

<p>启动监控后，服务器很快进驻不健康状态，提示问题为clock offset。查看后发现问题原因是没有启动ntpd服务，修复步骤
1.修改/etc/ntp.conf文件，增加以下设置
    server us.pool.ntp.org
2.启动ntp服务
    service ntpd start
3.验证
    ntpdc -c loopinfo
4.将ntpd服务添加到启动中
    chkconfig ntpd on</p>

<p>HBase  Canary 用于检测HBase 系统的状态。它对指定表的每一个region 抓取一行，来探测失败或者延迟。
默认情况下启动该服务，造成cpu持续高负载
修改/usr/lib64/cmf/service/hbase/hbase.sh文件，注释掉hbase_canary调用后，cpu负载下降
    if [ "regionserver" = "$1" -a -n "$CANARY_TIMEOUT" ]; then</p>

<h1>
<a id="hbase_canary-" class="anchor" href="#hbase_canary-" aria-hidden="true"><span class="octicon octicon-link"></span></a>hbase_canary &amp;</h1>

<pre><code>    echo 1
fi
</code></pre>

<p>-------------------------------------------------------2014 3 04-------------------------------------------------------------
该写导入mysql类，增强通用性</p>

<p>开始实践新环境hbase+mapreduce任务。
1.开始阶段遇到找不到hbase类的问题，修改/usr/lib/hadoop/bin/hadoop
    for f in /usr/lib/hbase/lib/*.jar; do
        CLASSPATH=${CLASSPATH}:$f;
    done
    明天学习任务启动机制！！
2.执行hadoop jar mapreduce.CountAge 遇到权限问题，切换到hdfs用户后解决
3.遇到域名解析错误，暂时没找到解决方法，但是意外发现不影响运行。
4.对600w数据进行测试，统计时间大概为8分钟，预计2000w数据不超过半小时，似乎性能好了很多。继续导入数据，增大测试数据集。</p>

<p>------------------------------------------------------2014 3 05-------------------------------------------------------------
早晨一来发现两个问题，第一导入数据的程序不是最新版，还是无法正常生产日志。第二个问题，76的regionserver挂了，检查日志，发现是因为传说中的gc问题。
测试大概一千万数据的年龄统计，用时9m42s。似乎效率提高了很多，详细测试有待继续
下午的任务
1.学习map/reduce作业提交机制
2.下载zookeeper和spark的书
3.写一个map/reduce程序，统计hbase一共多少行，现在统计太慢了，采用配置化或者参数形式，增加通用性。
4.有机会看一下内存回收机制</p>

<p>map.entry</p>

<p>------------------------------------------------------2014 3 06-------------------------------------------------------------
上午看了会hbase表设计，有点纠结solr的设置，所以跟踪了一下
前台启动脚本调用
/usr/lib64/cmf/service/solr/solr.sh
solr.sh调用
/usr/lib/solr/bin/solrd
solrd中对tomcat的参数进行了设置，包括hdfs的相关设置，并调用catalina.sh启动tomcat
看来cloudera采用的方法为不修改默认的配置文件，而是在运行时增加参数的方法
根据官方文档还可以采用修改solrconfig.xml的方式</p>

<p>为集群建立共享文件夹，设置在88的/root/share下
编写第一个coprocessor</p>

<p>alter 'weizhang2', METHOD =&gt; 'table_att','coprocessor' =&gt; 'file:///root/share/myCoprocessor.jar|hbase.MyObserver|1001|'
 put 'weizhang2','123','info:value','123'
alter 'weizhang2', METHOD =&gt; 'table_att','coprocessor' =&gt; 'file:///root/myCoprocessor.jar|hbase.MyObserver|USER'</p>

<p>alter 't1', METHOD =&gt; 'table_att_unset', NAME =&gt; 'coprocessor$1'</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
